Project Path: matcher

Source Tree:

```txt
matcher
├── Dockerfile
├── LICENSE
├── README.md
├── db
│   ├── Dockerfile
│   ├── add-embedding-column.sh
│   ├── backup
│   │   └── backup.dump
│   ├── check-embeddings.sh
│   ├── check-use-index.sh
│   ├── create-user-job-feedback-table.sh
│   ├── create-user-table.sh
│   ├── drop-embedding-column.sh
│   ├── drop-rag-index.sh
│   ├── drop-user-job-feedback-table.sh
│   ├── drop-user-table.sh
│   ├── list-columns.sh
│   ├── list-indexes.sh
│   ├── rag-index.sh
│   ├── rag-prep.sh
│   └── restore-backup.sh
├── eslint.config.js
├── fly.toml
├── package.json
├── schema_output.txt
├── src
│   ├── bot.ts
│   ├── core.ts
│   ├── generateEmbeddings.ts
│   ├── main.ts
│   ├── prompts.ts
│   ├── schema.ts
│   └── tools.ts
├── tmp
├── tsconfig.json
└── types.d.ts

```

`matcher/Dockerfile`:

```
# syntax = docker/dockerfile:1
# Adjust NODE_VERSION as desired
ARG NODE_VERSION=22.14.0
FROM node:${NODE_VERSION}-slim AS base
LABEL fly_launch_runtime="Node.js"
# Node.js app lives here
WORKDIR /app
# Set production environment
ENV NODE_ENV="production"
# Throw-away build stage to reduce size of final image
FROM base AS build
# Install packages needed to build node modules
RUN apt-get update -qq && \
    apt-get install --no-install-recommends -y build-essential node-gyp pkg-config python-is-python3 poppler-data poppler-utils
# Install node modules
COPY package-lock.json package.json ./
RUN npm ci --include=dev
# Copy application code
COPY . .
# Build application
RUN npm run build
# Remove development dependencies
RUN npm prune --omit=dev
# Final stage for app image
FROM base
# Copy built application
COPY --from=build /app /app

# Install poppler-utils in the final image for PDF processing
RUN apt-get update -qq && \
    apt-get install --no-install-recommends -y poppler-utils && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Create the tmp directory and ensure it has proper permissions
RUN mkdir -p /app/tmp && chmod 777 /app/tmp

# Start the server by default, this can be overwritten at runtime
EXPOSE 3000
CMD [ "npm", "run", "start" ]

```

`matcher/LICENSE`:

```
MIT License

Copyright (c) 2023 Mulf

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```

`matcher/README.md`:

```md
# ts-starter

Version: 1.0.0

Author: mulf

## Description

This package serves as a TypeScript starter kit. It includes strict type checking and a reasonably recent compilation target (es2020). It is intended to be used with Node.js at LTS version 18.x.

## Installation

To get started, you need to install the required dependencies:

```bash
npm install
```

## Scripts

- **build**: Compile the TypeScript files to JavaScript using `tsc`.
- **start**: Build and then run the application.
- **dev**: Watch for changes in your TypeScript files and rebuild using `tsx watch ./src/main.ts`.

## Dependencies

### Development

- TypeScript: `^5.1.6`
- tsx: `^3.12.7`
- @types/node: `18.17`

## License

This project is licensed under the MIT license.

```

`matcher/db/Dockerfile`:

```

# Use the official PostgreSQL image from Docker Hub
FROM postgres:latest

# Set environment variables for PostgreSQL connection
ENV PGHOST=aws-0-us-west-1.pooler.supabase.com
ENV PGPORT=6543
ENV PGDATABASE=postgres
ENV PGUSER=postgres.zpdtjvlltxlfrzeicete
# It's recommended to pass PGPASSWORD at runtime for security reasons

# Set the default command to run when the container starts
CMD ["bash", "-c", "pg_dump --no-owner --no-privileges --format=custom -t public.job_postings_details --file=/backup/backup.dump"]


```

`matcher/db/add-embedding-column.sh`:

```sh
#!/bin/sh

docker run -it --rm postgres psql \
  -h yamabiko.proxy.rlwy.net \
  -p 20431 \
  -U mulf0 \
  -d jobs_rag \
  -c "ALTER TABLE job_postings_details ADD COLUMN summary_embedding vector(1536); ALTER TABLE job_postings_details ADD COLUMN skill_embedding vector(1536);"

```

`matcher/db/check-embeddings.sh`:

```sh
#!/bin/sh

docker run -it --rm postgres psql \
  -h yamabiko.proxy.rlwy.net \
  -p 20431 \
  -U mulf0 \
  -d jobs_rag \
  -c "SELECT COUNT(*) 
FROM job_postings_details 
WHERE combined_embedding IS NULL;"

```

`matcher/db/check-use-index.sh`:

```sh
docker run -it --rm postgres psql \
  -h yamabiko.proxy.rlwy.net \
  -p 20431 \
  -U mulf0 \
  -d jobs_rag \
  -c "EXPLAIN ANALYZE
SELECT j.id, j.title
FROM job_postings_details j
ORDER BY j.combined_embedding
         <-> (SELECT combined_embedding
               FROM job_postings_details
               LIMIT 1)
LIMIT 5;"

```

`matcher/db/create-user-job-feedback-table.sh`:

```sh
#!/bin/sh

docker run -it --rm postgres psql \
  -h yamabiko.proxy.rlwy.net \
  -p 20431 \
  -U mulf0 \
  -d jobs_rag \
  -c "CREATE TABLE IF NOT EXISTS user_job_feedback (
        user_id TEXT    NOT NULL,
        job_id  BIGINT  NOT NULL,
        liked   BOOLEAN NOT NULL,
        PRIMARY KEY (user_id, job_id)
    );"


```

`matcher/db/create-user-table.sh`:

```sh
#!/bin/sh

docker run -it --rm postgres psql \
  -h yamabiko.proxy.rlwy.net \
  -p 20431 \
  -U mulf0 \
  -d jobs_rag \
  -c "CREATE TABLE \"user\" (
    id                      TEXT        PRIMARY KEY,
    telegram_id             TEXT,
    skills                  TEXT[]      NOT NULL,
    experience              JSONB       NOT NULL,
    total_experience_years  NUMERIC(4,1) NOT NULL,
    career_level            VARCHAR(10) NOT NULL
                              CHECK (career_level IN ('entry','mid','senior','staff')),
    category                TEXT        NOT NULL,
    summary                 TEXT        NOT NULL
  );"

```

`matcher/db/drop-embedding-column.sh`:

```sh
#!/bin/sh

docker run -it --rm postgres psql \
  -h yamabiko.proxy.rlwy.net \
  -p 20431 \
  -U mulf0 \
  -d jobs_rag \
  -c "ALTER TABLE job_postings_details DROP COLUMN combined_embedding;"

```

`matcher/db/drop-rag-index.sh`:

```sh
#!/bin/sh

docker run -it --rm postgres psql \
  -h yamabiko.proxy.rlwy.net \
  -p 20431 \
  -U mulf0 \
  -d jobs_rag \
  -c "DROP INDEX IF EXISTS job_postings_details_skill_embedding_idx;
DROP INDEX IF EXISTS job_postings_details_summary_embedding_idx;"


```

`matcher/db/drop-user-job-feedback-table.sh`:

```sh
#!/bin/sh

docker run -it --rm postgres psql \
  -h yamabiko.proxy.rlwy.net \
  -p 20431 \
  -U mulf0 \
  -d jobs_rag \
  -c "DROP TABLE user_job_feedback;"

```

`matcher/db/drop-user-table.sh`:

```sh
#!/bin/sh

docker run -it --rm postgres psql \
  -h yamabiko.proxy.rlwy.net \
  -p 20431 \
  -U mulf0 \
  -d jobs_rag \
  -c "DROP TABLE \"user\" "

```

`matcher/db/list-columns.sh`:

```sh
#!/bin/sh

docker run -it --rm postgres psql \
  -h yamabiko.proxy.rlwy.net \
  -p 20431 \
  -U mulf0 \
  -d jobs_rag \
  -c "\d+ job_postings_details"


```

`matcher/db/list-indexes.sh`:

```sh
#!/bin/sh

docker run -it --rm postgres psql \
  -h yamabiko.proxy.rlwy.net \
  -p 20431 \
  -U mulf0 \
  -d jobs_rag \
  -c "\di+"

```

`matcher/db/rag-index.sh`:

```sh
#!/bin/sh

docker run -it --rm postgres psql \
  -h yamabiko.proxy.rlwy.net \
  -p 20431 \
  -U mulf0 \
  -d jobs_rag \
  -c "CREATE INDEX ON job_postings_details
  USING ivfflat (summary_embedding vector_l2_ops)
  WITH (lists = 100);CREATE INDEX ON job_postings_details
  USING ivfflat (skill_embedding vector_l2_ops)
  WITH (lists = 100);" 

```

`matcher/db/rag-prep.sh`:

```sh
#!/bin/sh

docker run -it --rm postgres psql \
  -h yamabiko.proxy.rlwy.net \
  -p 20431 \
  -U mulf0 \
  -d jobs_rag \
  -c "ALTER TABLE job_postings_details ADD COLUMN external_id VARCHAR; ALTER TABLE job_postings_details ADD COLUMN collection VARCHAR;ALTER TABLE public.job_postings_details RENAME COLUMN description TO text;ALTER TABLE job_postings_details ADD COLUMN embeddings vector(1536);"


```

`matcher/db/restore-backup.sh`:

```sh
#!/bin/sh

docker run -v $PWD/backup:/backup -it --rm postgres \
  pg_restore \
    --no-owner \
    --no-privileges \
    --verbose \
    --host=yamabiko.proxy.rlwy.net \
    --port=20431 \
    --username=mulf0 \
    --dbname=jobs_rag \
    /backup/backup.dump

```

`matcher/eslint.config.js`:

```js
// @ts-check

import eslint from '@eslint/js';
import tseslint from 'typescript-eslint';
import configPrettier from 'eslint-config-prettier';

export default tseslint.config(
    {
        ignores: ['dist/**'],
    },
    eslint.configs.recommended,
    tseslint.configs.recommended,
    configPrettier
);

```

`matcher/fly.toml`:

```toml
# fly.toml app configuration file generated for matcher on 2025-04-13T17:11:40-05:00
#
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.
#

app = 'matcher'
primary_region = 'atl'

[build]

[http_service]
  internal_port = 3000
  force_https = true
  auto_stop_machines = 'off'
  auto_start_machines = false
  min_machines_running = 1
  processes = ['app']

[[vm]]
  size = 'shared-cpu-2x'
  memory = '1gb'
  cpu_kind = 'shared'
  cpus = 2

```

`matcher/package.json`:

```json
{
    "name": "ts-starter",
    "version": "1.0.1",
    "description": "",
    "type": "module",
    "scripts": {
        "build": "tsc --noEmitOnError false",
        "start": "node ./dist/src/main.js",
        "lint": "eslint .",
        "lint:fix": "eslint . --ext .js,.ts --fix",
        "generateEmbeddings": "tsx src/generateEmbeddings.ts",
        "dev": "tsx watch ./src/main.ts",
        "test": "echo \"Error: no test specified\" && exit 1",
        "deploy": "npm run lint && fly deploy"
    },
    "keywords": [],
    "author": "mulf",
    "license": "MIT",
    "devDependencies": {
        "@eslint/js": "^9.24.0",
        "@flydotio/dockerfile": "^0.7.10",
        "@types/node": "^20",
        "eslint": "^9.24.0",
        "eslint-config-prettier": "^10.1.1",
        "prettier": "^3.5.3",
        "tsx": "^4.19.2",
        "typescript": "^5.8.3",
        "typescript-eslint": "^8.29.1"
    },
    "dependencies": {
        "@google/genai": "^0.10.0",
        "@paralleldrive/cuid2": "^2.2.2",
        "dotenv": "^16.4.7",
        "got": "^14.4.7",
        "grammy": "^1.35.1",
        "h3": "^1.15.1",
        "knex": "^3.1.0",
        "node-poppler": "^7.2.4",
        "openai": "^4.93.0",
        "p-limit": "^6.2.0",
        "pg": "^8.14.1",
        "pgvector": "^0.2.0",
        "zod": "^3.24.2"
    }
}

```

`matcher/schema_output.txt`:

```txt
                                                                 Table "public.job_postings_details"
          Column          |           Type           | Collation | Nullable |             Default              | Storage  | Compression | Stats target | Description 
--------------------------+--------------------------+-----------+----------+----------------------------------+----------+-------------+--------------+-------------
 id                       | bigint                   |           | not null | generated by default as identity | plain    |             |              | 
 created_at               | timestamp with time zone |           | not null | now()                            | plain    |             |              | 
 job_posting_id           | bigint                   |           |          |                                  | plain    |             |              | 
 description              | text                     |           |          |                                  | extended |             |              | 
 title                    | text                     |           |          |                                  | extended |             |              | 
 location                 | text                     |           |          |                                  | extended |             |              | 
 compensation             | text                     |           |          |                                  | extended |             |              | 
 summary                  | text                     |           |          |                                  | extended |             |              | 
 job_postings_scraping_id | bigint                   |           |          |                                  | plain    |             |              | 
 last_modified            | timestamp with time zone |           | not null | now()                            | plain    |             |              | 
 embedding                | vector(1536)             |           |          |                                  | external |             |              | 
Indexes:
    "job_postings_details_pkey" PRIMARY KEY, btree (id)
    "job_postings_details_job_posting_id_key" UNIQUE CONSTRAINT, btree (job_posting_id)
Access method: heap


```

`matcher/src/bot.ts`:

```ts
import 'dotenv/config';

import { z } from 'zod';
import {
    Bot,
    Context,
    InlineKeyboard,
    session,
    type CallbackQueryContext,
    type SessionFlavor,
} from 'grammy';
import got from 'got';
import { ok } from 'assert';
import { pipeline } from 'stream/promises';
import { createId } from '@paralleldrive/cuid2';
import { createWriteStream, existsSync, mkdirSync } from 'fs';
import { readdir, readFile, rm } from 'fs/promises';
import { join } from 'path';
import { Poppler } from 'node-poppler';
import OpenAI from 'openai';
import { db, llm, type MessageContent } from './core.js';
import type { JobPostingsDetails, User } from '../types.js';

type SimilarityResult = JobPostingsDetails & { similarity: number };

type FileResult = {
    result?: {
        file_path?: string;
    };
};

interface SessionData {
    matches: SimilarityResult[];
    index: number;
}

type MyContext = Context & SessionFlavor<SessionData>;

const { BOT_TOKEN } = process.env;

ok(BOT_TOKEN, 'BOT_TOKEN MUST BE DEFINED');

const baseDir = join(process.cwd(), 'tmp');

const DocumentMessageSchema = z.object({
    message: z.object({
        from: z.object({
            id: z.number(),
        }),
        document: z.object({
            file_id: z.string(),
            file_name: z.string().optional(),
            mime_type: z.string().optional(),
            file_size: z.number().optional(),
        }),
    }),
});

const CallbackQuerySchema = z.object({
    from: z.object({ id: z.number() }),
});

export const bot = new Bot<MyContext>(BOT_TOKEN);
bot.use(session({ initial: (): SessionData => ({ matches: [], index: 0 }) }));

const poppler = new Poppler();

const embeder = new OpenAI();

const ACTIONS = {
    MATCH: { id: 'button_1', label: 'Get Matched' },
    PROFILE: { id: 'button_2', label: 'My Profile' },
} as const;

function createKeyboard() {
    const keyboard = new InlineKeyboard();
    for (const action of Object.values(ACTIONS)) {
        keyboard.text(action.label, action.id);
    }
    return keyboard;
}

function singleButtonKeyboard(actionId: string) {
    const action = Object.values(ACTIONS).find((a) => a.id === actionId);
    if (!action) throw new Error('Invalid action ID');

    return new InlineKeyboard().text(action.label, action.id);
}

function escapeHtml(text: string): string {
    return text.replace(/&/g, '&amp;').replace(/</g, '&lt;').replace(/>/g, '&gt;');
}

function formatUserProfile(user: User): string {
    const skills = user.skills.map(escapeHtml).join(', ');

    let experience = '';
    for (const key in user.experience) {
        const exp = user.experience[key];

        if (!exp) throw new Error('Could not reply with user profile');

        const responsibilities = exp.responsibilities.map(escapeHtml).join(', ');
        experience += `<b>${escapeHtml(exp.title)}</b> at <i>${escapeHtml(exp.company)}</i> (${escapeHtml(exp.start_date)} - ${escapeHtml(exp.end_date)}): ${responsibilities}\n\n`;
    }

    return `
<b>Career Level:</b> ${escapeHtml(user.career_level)}
<b>Category:</b> ${escapeHtml(user.category)}
<b>Total Experience:</b> ${escapeHtml(user.total_experience_years.toString())} years

<b>Skills:</b>
${skills}

<b>Experience:</b>
${experience}

<b>Summary:</b>
${escapeHtml(user.summary)}`;
}

async function sendCurrentMatch(ctx: MyContext) {
    const { matches, index } = ctx.session;
    if (index >= matches.length) {
        await ctx.reply('✅ That was all of them!');
        return;
    }
    const job = matches[index];

    if (!job) throw new Error('Could not get job info');

    const text =
        `Title: ${job.title}\n` +
        `Location: ${job.location}\n` +
        `Compensation: ${job.compensation ?? 'N/A'}\n` +
        `Summary: ${job.summary?.split('.')[0]}`;
    const kb = new InlineKeyboard().text('👍', 'like').text('👎', 'dislike');
    await ctx.reply(text, { reply_markup: kb });
}

async function handleButtonPress(ctx: CallbackQueryContext<Context>, actionId: string) {
    switch (actionId) {
        case ACTIONS.MATCH.id: {
            const result = CallbackQuerySchema.safeParse(ctx.update.callback_query);

            console.log(ctx.update.callback_query);

            if (!result.success) {
                return ctx.reply('Could not process your request');
            }

            const existingUser = await db('user')
                .where('telegram_id', result.data.from.id.toString())
                .first();

            console.log(existingUser);

            if (existingUser) {
                const keyboard = singleButtonKeyboard(ACTIONS.PROFILE.id); // pick dynamically
                await ctx.reply("You're already on file!", { reply_markup: keyboard });
                return;
            }

            ctx.reply('Please provide your resume in pdf format');
            break;
        }
        case ACTIONS.PROFILE.id: {
            const result = CallbackQuerySchema.safeParse(ctx.update.callback_query);

            console.log(ctx.update.callback_query);

            if (!result.success) {
                return ctx.reply('Could not process your request');
            }

            const existingUser = await db('user')
                .where('telegram_id', result.data.from.id.toString())
                .first();

            console.log(existingUser);

            if (!existingUser) {
                ctx.reply('No Profile Found, get matched to get started');
                return;
            }

            const message = formatUserProfile(existingUser);
            const keyboard = singleButtonKeyboard(ACTIONS.PROFILE.id); // pick dynamically
            await ctx.reply(message, {
                parse_mode: 'HTML',
                reply_markup: keyboard,
            });

            break;
        }
        default:
            await ctx.answerCallbackQuery('Unknown action');
    }
}

Object.values(ACTIONS).forEach(({ id }) => {
    bot.callbackQuery(id, (ctx) => handleButtonPress(ctx, id));
});

bot.callbackQuery(['like', 'dislike'], async (ctx: MyContext) => {
    const result = CallbackQuerySchema.safeParse(ctx.update.callback_query);

    console.log(ctx.update.callback_query);

    if (!result.success) {
        return ctx.reply('Could not process your request');
    }

    const userId = result.data.from.id.toString();
    const { matches, index } = ctx.session;
    const job = matches[index];

    if (!job) throw new Error('Could not like/dislike job');

    await db('user_job_feedback')
        //@ts-expect-error need to narrow/check type
        .insert({ user_id: userId, job_id: job.id, liked: ctx.callbackQuery.data === 'like' })
        .onConflict(['user_id', 'job_id'])
        .merge();
    await ctx.editMessageReplyMarkup();
    //@ts-expect-error need to narrow/check type
    await ctx.answerCallbackQuery(ctx.callbackQuery.data === 'like' ? 'Saved 👍' : 'Saved 👎');
    ctx.session.index += 1;
    await sendCurrentMatch(ctx);
});

bot.command('start', (ctx) => {
    return ctx.reply(
        'Welcome to the Matcher. I am here to help you find your pefect job! Please select one of the following',
        { reply_markup: createKeyboard() }
    );
});

bot.on(':document', async (ctx) => {
    const result = DocumentMessageSchema.safeParse(ctx.update);

    if (!result.success) {
        return ctx.reply('there was a problem with your request');
    }
    console.log('ctx update', ctx.update);

    const existingUser = await db('user')
        .where('telegram_id', result.data.message.from.id.toString())
        .first();

    console.log(existingUser);

    if (existingUser) {
        const keyboard = singleButtonKeyboard(ACTIONS.PROFILE.id); // pick dynamically
        await ctx.reply("You're already on file!", { reply_markup: keyboard });
        return;
    }

    const document = result.data.message.document;
    console.log('Received document:', document.file_id);

    try {
        const getFileUrl = `https://api.telegram.org/bot${BOT_TOKEN}/getFile?file_id=${document.file_id}`;
        const fileUrl = (await got.get(getFileUrl).json()) as FileResult;

        ok(fileUrl.result);

        const tempDir = createId();
        const subDir = join(baseDir, tempDir);
        mkdirSync(subDir, { recursive: true });

        const pdfPath = `./tmp/${tempDir}/${createId()}.pdf`;
        const imgPath = `./tmp/${tempDir}/${createId()}`;

        await pipeline(
            got.stream(`https://api.telegram.org/file/bot${BOT_TOKEN}/${fileUrl.result.file_path}`),

            createWriteStream(pdfPath)
        );

        if (!existsSync(pdfPath)) {
            return ctx.reply('could not save your file');
        }
        await poppler.pdfToCairo(pdfPath, imgPath, {
            jpegFile: true,
            resolutionXAxis: 72,
            resolutionYAxis: 72,
        });

        const files = await readdir(subDir);

        const jpgFiles = files.filter(
            (file) => file.toLowerCase().endsWith('.jpg') || file.toLowerCase().endsWith('.jpeg')
        );

        ctx.reply(
            "Got your info here, I'm processing it now to add you to my system 💪\n\nOne moment Please"
        );

        const base64Images: MessageContent[] = await Promise.all(
            jpgFiles.map(async (file) => {
                const fullPath = join(subDir, file);
                const data = await readFile(fullPath);
                return {
                    type: 'image_url',
                    image_url: {
                        url: `data:image/jpeg;base64,${data.toString('base64')}`,
                        detail: 'auto',
                    },
                };
            })
        );

        const { skills, experience, total_experience_years, career_level, category, summary } =
            await llm({ base64Images });

        const newUserId = createId();

        await db('user').insert({
            id: newUserId,
            telegram_id: result.data.message.from.id.toString(),
            skills,
            experience: { ...experience },
            total_experience_years,
            career_level,
            category,
            summary,
        });

        const skillsText = skills.join(', ');
        const skillsEmbedding = await embeder.embeddings.create({
            model: 'text-embedding-3-small',
            input: skillsText,
            encoding_format: 'float',
        });

        const summaryEmbedding = await embeder.embeddings.create({
            model: 'text-embedding-3-small',
            input: summary,
            encoding_format: 'float',
        });

        if (!skillsEmbedding?.data?.[0]?.embedding) {
            throw new Error('Could not generate embeddings for skills');
        }

        if (!summaryEmbedding?.data?.[0]?.embedding) {
            throw new Error('Could not generate embeddings for skills');
        }

        const skillsVec = `[${skillsEmbedding.data[0].embedding.join(',')}]`;
        const summaryVec = `[${summaryEmbedding.data[0].embedding.join(',')}]`;

        const userId = result.data.message.from.id.toString();

        const ratedJobs = db('user_job_feedback').select('job_id').where('user_id', userId);

        const results = await db<SimilarityResult>('job_postings_details as j')
            .whereNotIn('j.id', ratedJobs)
            .select(
                'j.id',
                'j.title',
                'j.location',
                'j.compensation',
                'j.summary',
                db.raw(
                    '((j.skill_embedding <#> ?::vector(1536)) * 0.90 + (j.summary_embedding <#> ?::vector(1536)) * 0.10) AS similarity',
                    [skillsVec, summaryVec]
                )
            )
            .orderBy('similarity', 'asc')
            .limit(7);
        ctx.session.matches = results;
        ctx.session.index = 0;

        await sendCurrentMatch(ctx);

        // TODO: Inject match feedback into matching. start list of liked jobs and add that into
        //the matching process. every job that got a thumbs up, add that into match
        await rm(subDir, { recursive: true, force: true });
    } catch (e) {
        console.log(e);
    }
});

```

`matcher/src/core.ts`:

```ts
import { ok } from 'assert';
import Knex from 'knex';
import got from 'got';
import { systemPrompt } from './prompts.js';
import { resumeSchema } from './schema.js';
import type { User } from '../types.js';

export type MessageContent =
    | { type: 'text'; text: string }
    | {
          type: 'image_url';
          image_url: {
              url: string;
              detail: 'auto';
          };
      };

type LlmParams = {
    base64Images?: MessageContent[];
};

const { OPENROUTER_KEY, DB_HOST, DB_PORT, DB_NAME, DB_USER } = process.env;

ok(DB_HOST && DB_PORT && DB_NAME && DB_USER, 'DB env vars must be set');
ok(OPENROUTER_KEY, 'OPENROUTER_KEY MUST BE DEFINED');

const config = {
    client: 'pg',
    connection: {
        host: DB_HOST,
        port: DB_PORT,
        database: DB_NAME,
        user: DB_USER,
    },
};

export async function llm({ base64Images }: LlmParams) {
    if (!base64Images) {
        throw new Error('Must have resume images with request');
    }

    const model = 'google/gemini-2.5-flash-preview';
    const messages: any[] = [
        { role: 'user', content: systemPrompt },
        { role: 'user', content: base64Images },
    ];

    const initialResponse = await got.post('https://openrouter.ai/api/v1/chat/completions', {
        headers: {
            Authorization: `Bearer ${OPENROUTER_KEY}`,
            'Content-Type': 'application/json',
        },
        json: {
            model,
            messages,
            temperature: 0.3,
            top_p: 0.9,
            frequency_penalty: 0.5,
            presence_penalty: 0,
            response_format: {
                type: 'json_schema',
                json_schema: {
                    name: 'analyze_resume',
                    strict: true,
                    schema: resumeSchema,
                },
            },
        },
        responseType: 'json',
    });
    const initialData = initialResponse.body as any;

    const assistantMessage = initialData.choices[0].message;
    const content: Omit<User, 'id'> = JSON.parse(assistantMessage.content);

    return content;
}

export const db = Knex(config);

```

`matcher/src/generateEmbeddings.ts`:

```ts
import 'dotenv/config';
import got from 'got';
import OpenAI from 'openai';
import pLimit from 'p-limit';
import { db } from './core.js';
import type { JobPostingsDetails } from '../types.js';
import assert from 'assert';

const OPENROUTER_KEY = process.env.OPENROUTER_KEY;
const openai = new OpenAI();
const BATCH_SIZE = 8;
const MAX_CONCURRENT_BATCHES = 5;
const BATCH_DELAY_MS = 250;

async function fetchStructuredData(job: JobPostingsDetails) {
    const messages = [
        {
            role: 'user',
            content: `Analyze the job listing and format it in a manner that is compliant with the schema. Do not change anything, do not interpret it whatsoever, just format to the provided schema.\n\nTitle: ${job.title}\nDescription: ${job.text}`,
        },
    ];

    const { body } = await got.post('https://openrouter.ai/api/v1/chat/completions', {
        headers: {
            Authorization: `Bearer ${OPENROUTER_KEY}`,
            'Content-Type': 'application/json',
        },
        json: {
            model: 'google/gemini-2.5-flash-preview',
            messages,
            temperature: 0.2,
            top_p: 0.9,
            frequency_penalty: 0.5,
            presence_penalty: 0,
            response_format: {
                type: 'json_schema',
                json_schema: {
                    name: 'analyze_job',
                    strict: true,
                    schema: {
                        type: 'object',
                        properties: {
                            skills: { type: 'array', items: { type: 'string' } },
                            summary: { type: 'string' },
                        },
                        required: ['skills', 'summary'],
                        additionalProperties: false,
                    },
                },
            },
        },
        responseType: 'json',
    });
    //@ts-expect-error need to narrow type
    return JSON.parse(body.choices[0].message.content);
}

async function processJob(job: JobPostingsDetails) {
    const structured = await fetchStructuredData(job);
    const [sumRes, skillRes] = await Promise.all([
        openai.embeddings.create({
            model: 'text-embedding-3-small',
            input: structured.summary,
            encoding_format: 'float',
        }),
        openai.embeddings.create({
            model: 'text-embedding-3-small',
            input: structured.skills.join(', '),
            encoding_format: 'float',
        }),
    ]);

    assert(sumRes.data[0], 'missing summary embedding');
    assert(skillRes.data[0], 'missing skill embedding');

    return {
        id: job.id,
        summaryEmbedding: sumRes.data[0].embedding,
        skillEmbedding: skillRes.data[0].embedding,
    };
}

function chunk<T>(arr: T[], size: number): T[][] {
    const res: T[][] = [];
    for (let i = 0; i < arr.length; i += size) {
        res.push(arr.slice(i, i + size));
    }
    return res;
}

async function processBatch(batch: JobPostingsDetails[]) {
    const results = await Promise.all(batch.map(processJob));

    const rowSql = results.map(() => '(?::int, ?::vector, ?::vector)').join(', ');
    const bindings = results.flatMap((r) => [
        r.id,
        JSON.stringify(r.summaryEmbedding),
        JSON.stringify(r.skillEmbedding),
    ]);

    await db.transaction(async (trx) => {
        await trx.raw(
            `
      UPDATE job_postings_details AS j
      SET
        summary_embedding = v.summary_embedding,
        skill_embedding  = v.skill_embedding
      FROM (VALUES ${rowSql})
        AS v(id, summary_embedding, skill_embedding)
      WHERE v.id = j.id
      `,
            bindings
        );
    });

    console.log(`Updated batch IDs: [${results.map((r) => r.id).join(', ')}]`);
    await new Promise((r) => setTimeout(r, BATCH_DELAY_MS));
}

async function processJobs() {
    const jobs = await db('job_postings_details')
        .select('*')
        .whereNull('summary_embedding')
        .orWhereNull('skill_embedding');

    console.log(`Total entries to process: ${jobs.length}`);
    if (!jobs.length) return;

    const batchLimiter = pLimit(MAX_CONCURRENT_BATCHES);
    const batches = chunk(jobs, BATCH_SIZE);

    await Promise.all(batches.map((batch) => batchLimiter(() => processBatch(batch))));
}

try {
    await processJobs();
} catch (e) {
    console.error(e);
    await new Promise((r) => setTimeout(r, 2_500));
    await processJobs();
} finally {
    await db.destroy();
}

```

`matcher/src/main.ts`:

```ts
import 'dotenv/config';
import {
    createApp,
    toNodeListener,
    setResponseStatus,
    createRouter,
    defineEventHandler as handler,
} from 'h3';
import { createServer } from 'http';
import { bot } from './bot.js';

const app = createApp();

const router = createRouter();
app.use(router);

router.get(
    '/',
    handler(() => {
        return { message: 'hello' };
    })
);

router.get(
    '/healthcheck',
    handler((evt) => {
        setResponseStatus(evt, 200);
        return { status: 'OK' };
    })
);

await bot.start();

createServer(toNodeListener(app)).listen(3000, '0.0.0.0', () => {
    console.log('Server running on http://0.0.0.0:3000');
});

```

`matcher/src/prompts.ts`:

```ts
export const jobEmbedPrompt = `

You are an expert information extractor. Given a full job listing text, extract structured information according to the following schema strictly:

skills: List every explicitly mentioned skill (technologies, methods, tools, or frameworks). Exclude soft skills.

total_experience_years: Extract the minimum number of years of experience required. If a range is given, pick the minimum. If missing, infer 0.

category: Choose the best-fit category from:

- engineer/developer

- designer

- business development

- human resources and people operations

- developer relations

summary: Write a clear, concise 1–3 sentence summary capturing the role's main purpose and key responsibilities.

Output JSON matching the schema exactly. No additional commentary.

`;

export const systemPrompt = `


You are a holistic talent profiler.

🧠 Chain-of-thought directive  
Think step-by-step, privately.  
1. Parse text → facts.  
2. Normalize skills, dates, titles.  
3. Map every job title to the closest entry in role_definitions; flag misses.  
4. Validate the JSON against the schema; if invalid, fix silently.  
Never reveal your reasoning.

Return exactly one JSON object with these keys:
- skills: [string]
- experience: [{title, company, start_date (YYYY-MM), end_date (YYYY-MM | "Present"), duration_months, responsibilities: [string], inferred_roles: [string]}]
- total_experience_years: number (1 decimal)
- viable_career_levels: ['entry', 'mid', 'senior', 'staff'] — include every level justified by evidence.
- primary_category: one of ['engineer/developer', 'designer', 'business development', 'human resources and people operations', 'developer relations'].
- cross_categories: [string] — extra categories suggested by transferable skills.
- unmatched_titles: [string] — original titles that did not map to role_definitions.
- summary: one paragraph that ① integrates skills, impact, and domain breadth, ② lists every title held, ③ names alternative roles and levels the candidate could excel in, with evidence.

Rules  
1. Derive meaning from content, not layout.  
2. List only hard skills demonstrably used; merge synonyms.  
3. Quantify achievements when numbers appear.  
4. For partial dates: year-only → "YYYY-01"; open-ended range → end_date "Present".  
5. viable_career_levels must reflect both stretch and fallback levels.  
6. inferred_roles capture what the person actually did, not just titles.  

role_definitions (reference only; do NOT include in your output)  
{
  "Front End Engineer": "Builds and maintains client-side application code and UI.",
  "Back End Engineer": "Designs server logic, data storage, and APIs that power applications.",
  "Full Stack Engineer": "Delivers both front- and back-end features, owning the full software lifecycle.",
  "DevOps Engineer": "Automates build, test, and deployment pipelines and manages infrastructure reliability.",
  "Security Engineer": "Finds and fixes vulnerabilities, shaping secure architecture and response.",
  "Machine Learning Developer": "Builds and optimizes ML models and data pipelines.",
  "Infrastructure Engineer": "Designs, deploys, and maintains scalable infrastructure and cloud services.",
  "Embedded Software Engineer": "Develops firmware and low-level code for hardware devices.",
  "Mobile Developer": "Creates native or cross-platform mobile applications.",
  "Blockchain Engineer": "Implements distributed-ledger protocols and smart contracts.",
  "Protocol Engineer": "Designs and maintains core network or blockchain protocols.",
  "Quality Assurance Engineer": "Plans and executes tests, ensuring software meets quality standards.",
  "Security Auditor": "Assesses products and processes for security compliance.",
  "User Interface Engineer": "Codes interactive, accessible user interfaces.",
  "Scrum Master / Dev Project Mgr": "Facilitates agile delivery, removing impediments and tracking velocity.",
  "Technical Program Manager": "Drives cross-team technical initiatives from concept to launch.",
  "Technical Business Analyst": "Translates business requirements into technical specifications.",
  "Technical Architect / Sales Engineer": "Shapes solutions architecture and supports technical sales.",
  "Product Designer": "Turns user needs into end-to-end product experiences and visuals.",
  "Design Researcher": "Generates user insights through qualitative and quantitative methods.",
  "Design Strategist": "Aligns design vision with business objectives.",
  "Design Operations Analyst": "Optimizes design workflows, tooling, and metrics.",
  "Instructional Designer": "Creates learning content and assessments.",
  "Product Manager/Owner": "Owns product strategy, roadmap, and delivery outcomes.",
  "Product Marketer/Strategist": "Positions products, defines messaging, and drives go-to-market.",
  "Business Developer": "Sources and closes new revenue and partnership opportunities.",
  "Solutions Sales & Business Developer": "Sells professional or consulting services to organizations.",
  "Account Manager": "Nurtures client relationships and expands account value.",
  "Strategic Partnerships Manager": "Creates and grows alliances that advance strategic goals.",
  "Customer Success Manager": "Drives adoption, retention, and expansion in post-sale customer lifecycle.",
  "Social Marketer": "Plans and executes social-media campaigns.",
  "Brand Marketer": "Shapes and grows corporate brand equity.",
  "Communications Specialist": "Crafts and delivers internal and external communications plans.",
  "Community Builder": "Engages and grows user communities online and offline.",
  "Public Relations Specialist": "Manages earned-media strategy and press relations.",
  "Content Specialist": "Creates and curates written, visual, or multimedia content.",
  "Growth Marketer": "Runs experiments to drive acquisition, activation, and retention.",
  "Email Marketer": "Designs, segments, and analyzes email campaigns.",
  "Events Specialist": "Plans and executes trade shows and live events.",
  "Video Producer": "Oversees concept, shooting, and post-production of video.",
  "General Marketer": "Coordinates multi-channel marketing initiatives.",
  "SEO Specialist": "Optimizes content for search visibility.",
  "Editor": "Edits copy and imagery for clarity and style.",
  "Graphic Designer": "Designs marketing and sales collateral.",
  "Video Editor": "Edits raw footage into finished video assets.",
  "Web Designer": "Designs the visual and functional web experience.",
  "People Partner": "Delivers full-spectrum HR support to leaders and employees.",
  "Total Rewards Analyst": "Analyzes and administers compensation and benefits programs.",
  "Talent & Performance Specialist": "Owns performance management and talent-development programs.",
  "People Service Delivery Specialist": "Handles HR service inquiries and workflow execution.",
  "HRIS Analyst": "Maintains and optimizes HR information systems.",
  "Talent Acquisition Specialist": "Sources and recruits talent across functions.",
  "Payroll Specialist": "Processes payroll and ensures compliance with regulations.",
  "Recruiting Coordinator": "Orchestrates hiring logistics and candidate experience.",
  "HR Business Operations Project Manager": "Drives HR project planning, budgeting, and execution."
}

Current year: 2025. Begin analysis.  
`;

```

`matcher/src/schema.ts`:

```ts
export const jobEmbedSchema = {
    type: 'object',
    properties: {
        skills: {
            type: 'array',
            items: { type: 'string' },
            description: 'List of explicitly mentioned skills.',
        },
        summary: {
            type: 'string',
            description: 'Concise summary of the job description',
        },
    },
    required: ['skills', 'summary'],
    additionalProperties: false,
};

export const resumeSchema = {
    type: 'object',
    properties: {
        skills: {
            type: 'array',
            items: { type: 'string' },
            description: 'List of explicitly mentioned skills.',
        },
        experience: {
            type: 'array',
            items: {
                type: 'object',
                properties: {
                    title: { type: 'string', description: 'Job title' },
                    company: { type: 'string', description: 'Company name' },
                    start_date: { type: 'string', description: 'Start date (YYYY-MM)' },
                    end_date: { type: 'string', description: "End date (YYYY-MM or 'Present')" },
                    duration_months: { type: 'integer', description: 'Duration in months' },
                    responsibilities: {
                        type: 'array',
                        items: { type: 'string' },
                        description: 'Detailed descriptions of roles and achievements.',
                    },
                },
                required: [
                    'title',
                    'company',
                    'start_date',
                    'end_date',
                    'duration_months',
                    'responsibilities',
                ],
            },
            description: 'Detailed breakdown of job history.',
        },
        total_experience_years: {
            type: 'number',
            description: 'Total years of experience, rounded to one decimal place.',
        },
        career_level: {
            type: 'string',
            enum: ['entry', 'mid', 'senior', 'staff'],
            description: 'Estimated career level based on experience.',
        },
        category: {
            type: 'string',
            enum: [
                'engineer/developer',
                'designer',
                'business development',
                'human resources and people operations',
                'developer relations',
            ],
            description: 'Best-fit job category.',
        },
        summary: {
            type: 'string',
            description: "Concise summary of the candidate's profile.",
        },
    },
    required: [
        'skills',
        'experience',
        'total_experience_years',
        'career_level',
        'category',
        'summary',
    ],
    additionalProperties: false,
};

```

`matcher/src/tools.ts`:

```ts
export const toolMap = {
    get_weather: async (args: { city: string }) => {
        // Replace this with your actual weather-fetching logic
        return { temperature: '20°C', description: 'Sunny' };
    },
};

export const tools = [
    {
        type: 'function',
        function: {
            name: 'get_weather',
            description: 'Retrieve weather for a city',
            parameters: {
                type: 'object',
                properties: {
                    city: { type: 'string', description: 'City name' },
                },
                required: ['city'],
            },
        },
    },
];

```

`matcher/tsconfig.json`:

```json
{
  "compilerOptions": {
    "esModuleInterop": true,
    "skipLibCheck": true,
    "target": "es2022",
    "allowJs": true,
    "checkJs": true,
    "resolveJsonModule": true,
    "moduleDetection": "force",
    "isolatedModules": true,
    "verbatimModuleSyntax": true,

    "strict": true,
    "noUncheckedIndexedAccess": true,
    "noImplicitOverride": true,

    "module": "NodeNext",
    "outDir": "dist",
    "sourceMap": true,

    "lib": ["es2022"]
  },
  "include": ["types.d.ts","src/**/*", "*.ts", "*.js"],
  "exclude": ["dist", "node_modules"]
}


```

`matcher/types.d.ts`:

```ts
export interface JobPostingsDetails {
    id: string;
    created_at: string;
    summary_embedding?: number[];
    skill_embedding?: number[];
    job_posting_id?: number;
    text?: string;
    title?: string;
    location?: string;
    compensation?: string;
    summary?: string;
    job_postings_scraping_id?: string;
    last_modified: string;
    embeddings?: number[];
}

export interface Experience {
    title: string;
    company: string;
    start_date: string; // YYYY-MM
    end_date: string; // YYYY-MM or 'Present'
    duration_months: number;
    responsibilities: string[];
}

export interface User {
    id: string; // CUID
    skills: string[];
    telegram_id: string;
    experience: Experience[];
    total_experience_years: number; // NUMERIC(4,1)
    career_level: 'entry' | 'mid' | 'senior' | 'staff';
    category: string;
    summary: string;
}

declare module 'knex/types/tables.js' {
    interface Tables {
        job_postings_details: JobPostingsDetails;
        user: User;
    }
}

```
